
<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width,initial-scale=1">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link href="https://fonts.googleapis.com/css2?family=Inconsolata&family=Inter:wght@400&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">
	<link rel="stylesheet" href="style.css">
	<title>Sujay Bajracharya</title>
</head>
<div class="header">
	<h2 id='name'>Sujay Bajracharya</h2>
	<p>I'm a grad student at the Robotics Institute in Carnegie Mellon University working on cloth manipulation and Reinforcement Learning</p>
	<br>
</div>

<div id="navbar">
  <a class="active" href="javascript:void(0)">Projects</a>
  <a href="files/SujayBajracharya_Resume.pdf">Resume</a>
  <a href="https://linkedin.com/in/sujay-bajracharya/">LinkedIn</a>
  <a href="https://github.com/sujaymanb">Github</a>
  
</div>

<div class="content">
	<h2>Stuff that I worked on:</h2>

	<section>
		<h3>Learning cloth sliding using tactile sensing.</h3>

		<p>In previous works, tactile sensing has been mainly used in manipulation to get some information about the material of the object that is being manipulated, in order to, for example, prevent slip and improve the quality of the grasp. Humans can leverage tactile information for other dexterous manipulation tasks like manipulating an object without relying on vision using sliding. Comparing to different modalities of perception, tactile sensing has some advantages over vision. It provides location and geometry information of the cloth portion being grasped, which would be otherwise occluded when relying on vision only. Tactile sensors can provide undistubed information about the status of the current grasp. Possible applications of learning skills such as sliding are: cloth folding, rope manipulation, etc.</p>

		<p>We are using a sawyer equipped with a WSG 32 gripper w/ WSG DSA tactile sensor fingers. The task that we are trying to do is this: One point of the cloth is fixed, the gripper grasps the cloth edge near this point and slides to get to the end of the cloth. The policy is trained on real robot using TD3.</p>
		</br>
		<div class="auto-resizable-iframe">
		<div>
		<iframe src="https://www.youtube.com/embed/BCk0LQ08qsQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</div>
		</div>
		</br></br>
	</section>

	<section>
		<h3>Augmenting Knowledge through Statistical, Goal-oriented Human-Robot Dialog</h3>
		<i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2019)</i></br>
		<span>Saeid Amiri, Sujay Bajracharya, Cihangir Goktolga, Jesse Thomason, Shiqi Zhang</span></br>
		<a href="https://arxiv.org/abs/1907.03390">https://arxiv.org/abs/1907.03390</a>
		</br>
		<img src="images/dialogmgr.png" class="block center"></br>
		<p>Some robots can interact with humans using natural language, and identify service requests through human-robot dialog. However, few robots are able to improve their language capabilities from this experience. In this paper, we develop a dialog agent for robots that is able to interpret user commands using a semantic parser, while asking clarification questions using a probabilistic dialog manager. This dialog agent is able to augment its knowledge base and improve its language capabilities by learning from dialog experiences, e.g., adding new entities and learning new ways of referring to existing entities. We have extensively evaluated our dialog system in simulation as well as with human participants through MTurk and real-robot platforms. We demonstrate that our dialog agent performs better in efficiency and accuracy in comparison to baseline learning agents.</p>
		</br>
		<div class="auto-resizable-iframe">
		<div>
		<iframe width="1120" height="630" src="https://www.youtube.com/embed/DFB3jbHBqYE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen class="block center"></iframe>
		</div>
		</div>
		</br></br>
	</section>

	<section>
		<p><h3>Simultaneous Intention Estimation and Knowledge Augmentation via Human-Robot Dialog</h3>
		<i>RSS 2018 Workshop on Models and Representations for Natural Human-Robot Communication</i></br>
		<span>Sujay Bajracharya, Saeid Amiri, Jesse Thomason, Shiqi Zhang</span></br>
		<a href="https://arxiv.org/pdf/1907.03390.pdf">https://arxiv.org/abs/1907.03390</a>
		</p>
		</br>
	</section>

	<section>
		<h3>Autonomous Meal Assistance Robot</h3>
		<i>REU: University of Wisconsin-Stout</i></br>
		<div class="flex-center"><img src="images/arm.png"><img src="images/face.png"></iframe></div>
		<p>Robotic arms used for meal assistance can help improve autonomy and quality of life for people with disabilites. However, controlling such a system is often  difficult for individuals with upper-body disabilities. We created a meal assistance robot capable of navigating to and from a user’s mouth and a bowl of food through the use of landmarks and facial tracking. The system can be controlled with both facial gestures and speech commands, allowing it to be used by individuals with a wide range of disabilities. We also tested our system with nine able-bodied participants, with each participant running three trials of eating soup and dry food for both the manual and autonomous system. Our hypothesis was that our autonomous system would be faster, easier to use and generally preferred over manual mode. The data showed that our system was in fact preferred and easier to use; however, only for difficult tasks was it proven to be faster than manual control. </p>

		<p>We utilized a Kinova JACO2 robotic arm and Microsoft Kinect camera.  We used facial tracking, gesture recognition and speech recognition to give commands to robot making feeding easier for people with upper body mobility issues who cannot easily control the robot otherwise.</p>
		</br>
		<div class="auto-resizable-iframe">
		<div>
		<iframe width="1120" height="630" src="https://www.youtube.com/embed/kkbPEdshNx8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</div>
		</div>
	</br>
	</br>

	</section>

	<section>
		<h3>UAV-UGV Collaboration</h3>
		<i>Cleveland State University</i></br>
		<p>Implemented autonomous UAV landing on UGV platform using vision.  Implemented communication and task synchronization between UAV and UGV for collaboration to complete high level task from human input.</p>
		</br>
		<div class="auto-resizable-iframe">
		<div>
		<iframe width="1120" height="630" src="https://www.youtube.com/embed/NZQ-CGzPjCY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></br>
		</div>
		</div>
		<img class="poster" src="images/UAVUGVfinal.png" width=600px class="block center">
		</br>
		</br>
	</section>

</div>

<footer>
	<a href="https://linkedin.com/in/sujay-bajracharya/"><i class="fa fa-linkedin" aria-hidden="true" style ="margin-right:10px;"></i></a>
	<a href="https://github.com/sujaymanb"><i class="fa fa-github" aria-hidden="true" style ="margin-right:10px;"></i></a>
	<p><span>Copyright © - 2019</span></p>
</footer>

<script>
window.onscroll = function() {myFunction()};

var navbar = document.getElementById("navbar");
var sticky = navbar.offsetTop;

function myFunction() {
  if (window.pageYOffset >= sticky) {
    navbar.classList.add("sticky")
  } else {
    navbar.classList.remove("sticky");
  }
}
</script>
</html>
